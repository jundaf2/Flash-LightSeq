# Flash-LightSeq
A hybridization of Light-seq Transformer model and Flash-Attention.

Download the Transformer model weight from LightSeq and add them to `model_weights/`
